qqnorm(cond2$logRT)
qqnorm(cond2$sqrtRT)
qqnorm(cond2$RT)
round(stat.desc(cond2$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$RT,basic = FALSE, norm = TRUE),3)
yuen( formula = Reaction.Time ~ Condition , data = data)
install.packages("WRS2")
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loeading the packages we'll most likely use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# Then we'll read our data
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loeading the packages we'll most likely use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# Then we'll read our data
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loeading the packages we'll most likely use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# Then we'll read our data
yuen( formula = Reaction.Time ~ Condition , data = data)
yuen(formula = Reaction.Time ~ Condition , data = data)
yuen(formula = Reaction.Time ~ Condition , data = t_test_data)
yuen(formula = Reaction.Time ~ Condition , data = t_test_data)
ggplot(data, aes( x = Condition ,y = Reaction.Time ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
ggplot(t_test_data, aes( x = Condition ,y = Reaction.Time ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
yuen(Reaction.Time ~ Condition , data = t_test_data)
library(lme4)
View(t_test_data)
m = lm(Reaction.Time ~ Condition, t_test_data)
summary(m)
yuen(Reaction.Time ~ Condition , data = t_test_data)
ggplot(t_test_data, aes( x = Condition ,y = Reaction.Time ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loeading the packages we'll most likely use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# First we start out by choosing a set of data from our reading experiment - we chose Thomas's data
data <- read.csv("logfile_Thomas Hagen Hansen2.csv", sep = ",")
# We start out making a qqplot over reactiontime
qqnorm(data$Reaction.Time)
# that doesn't look normally distributed
#We'll try to get a better look by making a histogram over reactiontime
ggplot(data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..))+ #add the histogram layer with density on the y-axis
stat_function(fun = dnorm,
args = list(mean = mean(data$Reaction.Time),
sd = sd(data$Reaction.Time)), color = "orange")+
labs(title = "Histogram with reaction time and condition")
#This also shows us that the data aren't "really" normally distributed
round(stat.desc(data$Reaction.Time, basic = FALSE, norm = TRUE),3)
# We try to do 3 different transformations on our reading data to get it normally distributed
# We start out with a logarithmic transformation
data$log <- log(data$Reaction.Time)
qqnorm(data$log, main = "QQ-Plot of Logarithmic transformation")
data$sqrt <- sqrt(data$Reaction.Time)
qqnorm(data$sqrt, main = "QQ-Plot of Squareroot transformation")
data$RT <- 1/(data$Reaction.Time)
qqnorm(data$RT, main = "QQ-Plot of Reciprocal transformation")
# # The reciprocal qqplot seems fairly normal - We'll go ahead with these transformed data and check to be sure that it's really normally distributed, we'll return normal distribution statistics on our Log transformation:
round(stat.desc(data$RT, basic = FALSE, norm = TRUE),3)
#Our skew.2SE and kurt.2SE is -2.537 and  1.211 seperately, which is lower than -1 and higher than 1, and this means that the skewness and kurtosis is significantly different from zero
# So.. Good thing we checked! Now we found out that the reciprocal data still wasn't normally distributed, so we'll use the more robust non-parametric spearmans-test in the coming correlational studies
# Correlational study 1 - Reaction Time and Word Length
# First we'll make a new column in our data with the mutate function, and use the stri_length function to count the letters in all the words
#But we do that, we need to remove all the special caracters and make the words Uppercase
data$clean_words <- str_replace_all(data$Word, pattern = "[[:punct:]]", replacement = "")
data$clean_words <- toupper(data$clean_words) #make words upper case
data <- mutate(data, Word_length = stri_length(data$clean_words))
# Now we'll make a correlation test to see if the reactiontime and the word length are correlated, and we'll use spearmans method since we didn't get any normal distributions with our earlier transformations.
cor.test(data$Word_length , data$Reaction.Time, method = "spearman")
#as can be seen in the correlation test, p-value is 0.007 and therefore below .05, so the relation between variables is significant. Rho is .196 which is above 0.1 which means we have a small effect -> Rho^2 is 0.04, which means that word_length captures 4% of the variance
ggplot(data, aes(x = Word_length, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#and as seen on the graph, we can see that by increasing word length we also increase the reaction time
cor.test(data$Word_length , data$Reaction.Time, method = "spearman")
r^2=0.1963504^2
r^2
roh squared=0.1963504^2
roh squared <- 0.1963504^2
((0.1963504^2)*100)
ggplot(data, aes(x = Word_length, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
word_list <- data$clean_words #turn column into a list
word_list$clean_words <- toupper(word_list$clean_words)
data$clean_words <- toupper(data$clean_words)
View(data)
cor.test(x= merged_df$freq, y = merged_df$Reaction.Time, method = "spearman")
# Correlational study 1 - Reaction Time and Word Length
# First we'll make a new column in our data with the mutate function, and use the stri_length function to count the letters in all the words
#But we do that, we need to remove all the special caracters and make the words Uppercase
data$clean_words <- str_replace_all(data$Word, pattern = "[[:punct:]]", replacement = "")
data$clean_words <- toupper(data$clean_words) #make words upper case
data <- mutate(data, Word_length = stri_length(data$clean_words))
# Now we'll make a correlation test to see if the reactiontime and the word length are correlated, and we'll use spearmans method since we didn't get any normal distributions with our earlier transformations.
cor.test(data$Word_length , data$Reaction.Time, method = "spearman")
#calculate r^2
((0.1963504^2)*100)
#as can be seen in the correlation test, p-value is 0.007 and therefore below .05, so the relation between variables is significant. Rho is .196 which is above 0.1 which means we have a small effect -> Rho^2 is 0.04, which means that word_length captures 4% of the variance
ggplot(data, aes(x = Word_length, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#and as seen on the graph, we can see that by increasing word length we also increase the reaction time
# Correlational study 2 - Reaction Time and Word Frequency
word_list <- data$clean_words #turn column into a list
word_list <- unique(word_list) #removes duplicates from text
word_string <-  paste( unlist(word_list), collapse=" ") #put a space in between all elements
word_string
freq <- read.csv("brown_frequency.csv", header = F, sep = ";",na.strings = "-")
#import the excel file into R
colnames(freq) = c("clean_words", "frequency") #rename the columns
merged_df <-merge(data, freq, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df$freq, y = merged_df$Reaction.Time, method = "spearman")
#it can be seen that p-value (.03) is below .05 which means that the relation is significant. Rho coeficient is negative (-0.16) which indicates that with higher frequency of words reaction time gets smaller, furthermore it indicates that we see a little effect. Rho^2 is .03, which means that
ggplot(merged_df, aes(x = frequency, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#graph confirms correlation analyses that reaction time gets smaller with higher frequency
cor.test(x= merged_df$freq, y = merged_df$Reaction.Time, method = "spearman")
((-0.1644064 ^2)*100)
((-0.1644064^2)*100)
((0.1644064^2)*100)
(((-0.1644064)^2)*100)
# Correlational study 3 - Reaction Time and Imaginability
#Copy string into MRC database with imaganability rating and manully copy into excel. Use the text to columns function to seperate the data and save output as CSV.
imagability <- read.csv("imagability.csv", header = F, sep = ";",na.strings = "-")
colnames(imagability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imagability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
#calculate r^2
(((-0.1644064)^2)*100)
#we can see that p-value is not significant and therefore we cannot observe relation between imagability and reaction time. Also, rho coeficient is too small to observe correlation.
ggplot(merged_df2, aes(x = imaginability, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
(((0.01679731)^2)*100)
yuen(Reaction.Time ~ Condition , data = t_test_data)
# First we start out by choosing a set of data from our reading experiment - we chose Thomas's data
data <- read.csv("logfile_Thomas Hagen Hansen2.csv", sep = ",")
# We start out making a qqplot over reactiontime
qqnorm(data$Reaction.Time)
# it doesn't look normally distributed
#We'll try to get a better look by making a histogram over reactiontime
ggplot(data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..))+
stat_function(fun = dnorm,
args = list(mean = mean(data$Reaction.Time),
sd = sd(data$Reaction.Time)), color = "orange")+
labs(title = "Histogram with reaction time and condition")
#This also shows us that the data aren't "really" normally distributed
round(stat.desc(data$Reaction.Time, basic = FALSE, norm = TRUE),3)
# We try to do 3 different transformations on our reading data to get it normally distributed
# We start out with a logarithmic transformation
data$log <- log(data$Reaction.Time)
qqnorm(data$log, main = "QQ-Plot of Logarithmic transformation")
data$sqrt <- sqrt(data$Reaction.Time)
qqnorm(data$sqrt, main = "QQ-Plot of Squareroot transformation")
data$RT <- 1/(data$Reaction.Time)
qqnorm(data$RT, main = "QQ-Plot of Reciprocal transformation")
# # The reciprocal qqplot seems fairly normal - We'll go ahead with these transformed data and check to be sure that it's really normally distributed, we'll return normal distribution statistics on our Log transformation:
round(stat.desc(data$RT, basic = FALSE, norm = TRUE),3)
#Our skew.2SE and kurt.2SE is -2.537 and  1.211 seperately, which is lower than -1 and higher than 1, and this means that the skewness and kurtosis is significantly different from zero
# So.. Good thing we checked! Now we found out that the reciprocal data still wasn't normally distributed, so we'll use the more robust non-parametric spearmans-test in the coming correlational studies
ggplot(data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..))+
stat_function(fun = dnorm,
args = list(mean = mean(data$Reaction.Time),
sd = sd(data$Reaction.Time)), color = "orange")+
labs(title = "Histogram with reaction time and condition")
round(stat.desc(data$Reaction.Time, basic = FALSE, norm = TRUE),3)
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loading the packages we'll use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# First we start out by choosing a set of data from our reading experiment - we chose Thomas's data
data <- read.csv("logfile_Thomas Hagen Hansen2.csv", sep = ",")
# We start out making a qqplot over reactiontime
qqnorm(data$Reaction.Time)
# it doesn't look normally distributed
#We'll try to get a better look by making a histogram over reactiontime
ggplot(data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..))+
stat_function(fun = dnorm,
args = list(mean = mean(data$Reaction.Time),
sd = sd(data$Reaction.Time)), color = "orange")+
labs(title = "Histogram with reaction time and condition")
#This also shows us that the data aren't "really" normally distributed
# At last we'll use the stat.desc function:
round(stat.desc(data$Reaction.Time, basic = FALSE, norm = TRUE),3)
# Which shows us, that yes. We suffer from both kurtosis and positive skew
# We try to do 3 different transformations on our reading data to get it normally distributed
# We start out with a logarithmic transformation
data$log <- log(data$Reaction.Time)
qqnorm(data$log, main = "QQ-Plot of Logarithmic transformation")
data$sqrt <- sqrt(data$Reaction.Time)
qqnorm(data$sqrt, main = "QQ-Plot of Squareroot transformation")
data$RT <- 1/(data$Reaction.Time)
qqnorm(data$RT, main = "QQ-Plot of Reciprocal transformation")
# # The reciprocal qqplot seems fairly normal - We'll go ahead with these transformed data and check to be sure that it's really normally distribute.
#We'll return normal distribution statistics on our Log transformation:
round(stat.desc(data$RT, basic = FALSE, norm = TRUE),3)
#Our skew.2SE and kurt.2SE is -2.537 and  1.211 seperately, which is lower than -1 and higher than 1 seperately, and this means that the skewness and kurtosis is still significantly different from zero
# So.. Good thing we checked! Now we found out that the reciprocal data still wasn't normally distributed, so we'll use the more robust non-parametric spearmans-test in the coming correlational studies
View(data)
data$clean_words <- str_replace_all(data$Word, pattern = "[[:punct:]]", replacement = "")
data$clean_words <- toupper(data$clean_words) #make words upper case
data <- mutate(data, Word_length = stri_length(data$clean_words))
cor.test(data$Word_length , data$Reaction.Time, method = "spearman")
ggplot(data, aes(x = Word_length, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
rho^2
rho = 0.3337761
rho^2
word_list <- data$clean_words #turn column into a list
word_list <- unique(word_list) #removes duplicates from text
word_string <-  paste( unlist(word_list), collapse=" ") #put a space in between all elements
word_string
freq <- read.csv("brown_frequency.csv", header = F, sep = ";",na.strings = "-")
View(freq)
colnames(freq) = c("clean_words", "frequency") #rename the columns
View(freq)
colnames(freq) = c("clean_words", "frequency") # we rename the columns in our new Dataframe
merged_df <-merge(data, freq, by = "clean_words") #then merge the new predictor with the old dataset
cor.test(x= merged_df$freq, y = merged_df$Reaction.Time, method = "spearman")
(((-0.1644064)^2)*100)
ggplot(merged_df, aes(x = frequency, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
imagability <- read.csv("imagability.csv", header = F, sep = ";",na.strings = "-")
colnames(imagability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imagability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
imaginability <- read.csv("imagability.csv", header = F, sep = ";",na.strings = "-")
colnames(imaginability) = c("clean_words", "imaginability) #rename the columns
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
#calculate r^2
(((0.01679731)^2)*100)
#0.03%
#we can see that p-value is not significant and therefore we cannot observe relation between imagability and reaction time. Also, rho coeficient is too small to observe correlation.
ggplot(merged_df2, aes(x = imaginability, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
```
colnames(imaginability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
imaginability <- read.csv("imagability.csv", header = F, sep = ";",na.strings = "-")
colnames(imaginability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
imaginability <- read.csv("imaginability.csv", header = F, sep = ";",na.strings = "-")
colnames(imaginability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
View(merged_df2)
knitr::opts_chunk$set(echo = TRUE)
# First we start out by loading the packages we'll use:
library(tidyverse) ; library(stringr) ; library(stringi) ; library(pastecs) ; library(WRS2)
# Then we set the Working Directory
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/Portfolio 3")
# First we start out by choosing a set of data from our reading experiment - we chose Thomas's data
data <- read.csv("logfile_Thomas Hagen Hansen2.csv", sep = ",")
# We start out making a qqplot over reactiontime
qqnorm(data$Reaction.Time)
# it doesn't look normally distributed
#We'll try to get a better look by making a histogram over reactiontime
ggplot(data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..))+
stat_function(fun = dnorm,
args = list(mean = mean(data$Reaction.Time),
sd = sd(data$Reaction.Time)), color = "orange")+
labs(title = "Histogram with reaction time and condition")
#This also shows us that the data aren't "really" normally distributed
# At last we'll use the stat.desc function:
round(stat.desc(data$Reaction.Time, basic = FALSE, norm = TRUE),3)
# Which shows us, that yes. We suffer from both kurtosis and positive skew
# We try to do 3 different transformations on our reading data to get it normally distributed
# We start out with a logarithmic transformation
data$log <- log(data$Reaction.Time)
qqnorm(data$log, main = "QQ-Plot of Logarithmic transformation")
data$sqrt <- sqrt(data$Reaction.Time)
qqnorm(data$sqrt, main = "QQ-Plot of Squareroot transformation")
data$RT <- 1/(data$Reaction.Time)
qqnorm(data$RT, main = "QQ-Plot of Reciprocal transformation")
# # The reciprocal qqplot seems fairly normal - We'll go ahead with these transformed data and check to be sure that it's really normally distribute.
#We'll return normal distribution statistics on our Log transformation:
round(stat.desc(data$RT, basic = FALSE, norm = TRUE),3)
#Our skew.2SE and kurt.2SE is -2.537 and  1.211 seperately, which is lower than -1 and higher than 1 seperately, and this means that the skewness and kurtosis is still significantly different from zero
# So.. Good thing we checked! Now we found out that the reciprocal data still wasn't normally distributed, so we'll use the more robust non-parametric spearmans-test in the coming correlational studies
# Correlational study 1 - Reaction Time and Word Length
# First we'll make a new column in our "Thomas" data with the mutate function, and use the stri_length function to count the letters in all the words
#But before we do that we need to remove all the special caracters and make the words Uppercase, so we later on can get some statistics from the 'MRC Psycholinguistic Database'
data$clean_words <- str_replace_all(data$Word, pattern = "[[:punct:]]", replacement = "")
data$clean_words <- toupper(data$clean_words) #make words upper case
data <- mutate(data, Word_length = stri_length(data$clean_words))
# Now we'll make a correlation test to see if the reactiontime and the word length are correlated, and we'll use spearmans method since we didn't get any normal distributions with our earlier transformations of the Thomas data.
cor.test(data$Word_length , data$Reaction.Time, method = "spearman")
# we calculate r^2
((0.1963504^2)*100)
#as can be seen in the correlation test, p-value is .007 and therefore below .05, so the relation between variables is significant. Rho is .196 which is above 0.1 which means we have a small effect -> Rho^2 is 0.04, which means that word_length captures 4% of the variance in Thomas's reaction times
# We'll now show the relation with a scatterplot and and a linear model.
ggplot(data, aes(x = Word_length, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#as seen on the graph, we can see that by increasing word length we increase the reaction time
# Correlational study 2 - Reaction Time and Word Frequency
#Here we make a word_string of our variable clea_words taht we can plug into the UWC psycholinguistic database
word_list <- data$clean_words #turn column into a list
word_list <- unique(word_list) #removes duplicates from text
word_string <-  paste( unlist(word_list), collapse=" ") #put a space in between all elements
word_string
#We copy the string into MRC database with brown-frequency ratings, manually copy it into excel, Use the text to columns function to seperate the data, and finish of saving the output as CSV.
# then we read the CSV file
freq <- read.csv("brown_frequency.csv", header = F, sep = ";",na.strings = "-")
colnames(freq) = c("clean_words", "frequency") # we rename the columns in our new Dataframe
merged_df <-merge(data, freq, by = "clean_words") #then merge the new predictor with the old dataset
#And finish off runing a cor.test on the browns word frequency and Reactiontime
cor.test(x= merged_df$freq, y = merged_df$Reaction.Time, method = "spearman")
# we calculate r^2
(((-0.1644064)^2)*100)
#it can be seen that p-value (.03) is below .05 which means that the relation is significant. Rho is negative (-0.16) which indicates that with higher frequencyscores follow smaller reactiontimes, furthermore it indicates that we see a little effect. Rho^2 is 2.7, which means that we have incaptured 2.7% of the variance
# We'll now show the relation with a scatterplot and and a linear model.
ggplot(merged_df, aes(x = frequency, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
#as seen on the graph the reaction time is negatively correlated with frequencyscore
# Correlational study 3 - Reaction Time and Imaginability
#We copy the string into MRC database with imaginability ratings, manually copy it into excel, Use the text to columns function to seperate the data, and finish of saving the output as CSV.
# then we read the CSV file
imaginability <- read.csv("imaginability.csv", header = F, sep = ";",na.strings = "-")
colnames(imaginability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x= merged_df2$imaginability, y = merged_df2$Reaction.Time, method = "spearman")
#calculate r^2
(((0.01679731)^2)*100)
#0.03%
#we can see that p-value is not significant and therefore we cannot observe relation between imagability and reaction time. Also, rho coeficient is too small to observe correlation.
ggplot(merged_df2, aes(x = imaginability, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
imaginability <- read.csv("imaginability.csv", header = F, sep = ";",na.strings = "-")
colnames(imaginability) = c("clean_words", "imaginability") #rename the columns
merged_df2 <-merge(data, "imaginability", by = "clean_words") #merge new predictor with the old dataset
merged_df2 <-merge(data, imaginability, by = clean_words) #merge new predictor with the old dataset
merged_df2 <-merge(data, imaginability, by = "clean_words") #merge new predictor with the old dataset
cor.test(x = merged_df2 $imaginability, y = merged_df2$Reaction.Time, method = "spearman")
cor.test(x = merged_df2 $imaginability, y = merged_df2$Reaction.Time, method = "spearman", exact = NULL, conf.level = 0.95)
(((0.01679731)^2)*100)
ggplot(merged_df2, aes(x = imaginability, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
ggplot(merged_df, aes(x = frequency, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
ggplot(merged_df2, aes(x = imaginability, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
ggplot(merged_df, aes(x = frequency, y = Reaction.Time )) +
geom_point()+
geom_smooth(method = lm)
cor.test(x = merged_df2 $imaginability, y = merged_df2$Reaction.Time, method = "spearman")
cond1 <- filter(t_test_data, Condition == 1) #now we create new dataframe only with condition 1
cond2 <- filter(t_test_data, Condition == 2)# then only with condition 2
filenames <- list.files(pattern = "logfile*")
t_test_data = data.frame() #create a new empty df for this second part of the portfolio
for (i in filenames) {	#the we loop over the list of files
file = read.csv(i)	#we import the current file
t_test_data = rbind(t_test_data, file[162,])
#and append the 162th line of the current file to our new df 't_test_data'using the rbind function
}
cond1 <- filter(t_test_data, Condition == 1) #now we create new dataframe only with condition 1
cond2 <- filter(t_test_data, Condition == 2)# then only with condition 2
ggplot(t_test_data, aes(x = Reaction.Time))+
geom_histogram(aes(y = ..density..), binwidth = 0.06)+
stat_function(fun = dnorm, args = list(mean = mean(t_test_data$Reaction.Time), sd = sd(t_test_data$Reaction.Time)))+ facet_wrap(~ Condition)
qqnorm(cond1$Reaction.Time)
qqnorm(cond2$Reaction.Time)
round(stat.desc(cond1$Reaction.Time,basic = FALSE, norm = TRUE),3)
cond2 <-  mutate(cond2, logRT = log(Reaction.Time), sqrtRT = sqrt(Reaction.Time), RT = 1/(Reaction.Time))
qqnorm(data$Reaction.Time)
qqnorm(data$Reaction.Time, main = "QQ-Plot of over Reactiontime")
qqnorm(cond1$Reaction.Time, main = "QQ-Plot of reactiontime in Condition 1")
qqnorm(cond2$Reaction.Time, main = "QQ-Plot of reactiontime in Condition 2")
qqnorm(cond2$logRT, main = "QQ-Plot of Logarithmic transformation")
qqnorm(cond2$sqrtRT, main = "QQ-Plot of Squareroot transformation")
qqnorm(cond2$RT, main = "QQ-Plot of Reciprocal transformation")
round(stat.desc(cond2$logRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$sqrtRT,basic = FALSE, norm = TRUE),3)
round(stat.desc(cond2$RT,basic = FALSE, norm = TRUE),3)
yuen(Reaction.Time ~ Condition , data = t_test_data)
ggplot(t_test_data, aes( x = Condition ,y = Reaction.Time ))+
geom_bar(stat = "summary", fun.y= mean)+
geom_errorbar(stat = "summary", fun.data = mean_se, width = 0.2)
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/work/CLasswork/Regression 7")
child_aggresion_data <- read.delim("ChildAggression.dat", sep = '')
Dietaggresion <- lm(child_aggresion_data$Aggression ~ child_aggresion_data$Diet)
summary(Dietaggresion)
library(tidyverse) ; library(ggplot2)
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/work/CLasswork/Adam Semanticmap - 8")
read.csv("Harry Potter.csv")
personality_data <- read.csv("Harry Potter.csv")
library(tidyverse) ; library(ggplot2)
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/work/CLasswork/Adam Semanticmap - 8")
personality_data <- read.csv("Harry Potter.csv")
View(personality_data)
ggplot(your_data, aes(word_nr, distance, label = word,  color = distance)) +
geom_point() +
geom_line() +
geom_text(check_overlap = T, position =  position_jitter(height = 2))+
ggtitle("Lineplot of semantic distance (trial by trial)")
library(tidyverse) ; library(ggplot2)
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/work/CLasswork/Adam Semanticmap - 8")
personality_data <- read.csv("Harry Potter.csv")
library(tidyverse) ; library(ggplot2)
setwd("/Users/FlowersnIce-cream/Google Drev/Hogwarts/R Studio/work/CLasswork/Adam Semanticmap - 8")
your_data <- read.csv("Harry Potter.csv")
ggplot(your_data, aes(word_nr, distance, label = word,  color = distance)) +
geom_point() +
geom_line() +
geom_text(check_overlap = T, position =  position_jitter(height = 2))+
ggtitle("Lineplot of semantic distance (trial by trial)")
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 5, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 0.2, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 7, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 10, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 5, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 20, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 30, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
your_data %>%
filter(distance != 100) %>%
ggplot(aes(distance, color = distance)) +
geom_histogram(binwidth = 5, aes(fill = ..count..)) +
scale_fill_gradient("count", low="orange", high="purple") +
labs(x = "Distance",
y = "Count",
title = "Semantic distance distribution") +
scale_x_continuous(breaks = seq(0,100,5))
ggplot(your_data, aes(x=time, y=distance, color=distance))+geom_point()+geom_smooth(method = "lm")
